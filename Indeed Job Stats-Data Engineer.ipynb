{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Program to analyze and find salary of a job profile</center>\n",
    "### <center>by Sushant Deshpande</center>\n",
    "\n",
    "I wrote this program so that it can scrape and analyze several jobs listed on Indeed.com or Indeed.ca in this case since I am in Canada and find the average salary of jobs. In addition to this, it also tells us which city in Canada has the maximum number of jobs and maximum salary. I have plotted all this using bar graph to make it visually easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's start by asking the user which job title he wants search for. First, we create a variable called <b> *job_title_1*</b> and store the user input in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_title_1 = input(\"Enter the Job Title: \")\n",
    "job_title_1 = \"Data Engineer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's import the required libries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import time\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way Indeed works is it takes the given user input, converts it to lower case and replaces the spaces with +. So we need to do the same.\n",
    "\n",
    "Let's convert the job title to lower case using <b>.lower()</b> method.\n",
    "\n",
    "Then, let's replace the spaces with + using <b>.repalce()</b> method.\n",
    "\n",
    "And then let's see how the output looks, <b>job_title_3</b> in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data+engineer'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title_2 = job_title_1.lower() # convert to lower case\n",
    "job_title_3 = job_title_2.replace(' ', '+') # replace space with +\n",
    "job_title_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, since we are searching for <b>Data Scientist</b> the output looks good, <b>*data+scientist*</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write the code that can scrape Indeed.ca with our <b>*job_title_3*</b>\n",
    "\n",
    "Here we insert job_title_3 into our url using concatenate feature in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25-February-2021'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Textual month, day and year\n",
    "d2 = today.strftime(\"%d-%B-%Y\")\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_engineer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title_lower = job_title_1.lower() # convert to lower case\n",
    "job_title_us = job_title_2.replace(' ', '_') # replace space with +\n",
    "job_title_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.indeed.ca/jobs?q='+job_title_3+'&l=canada'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the url that we just parsed, just to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.ca/jobs?q=data+engineer&l=canada'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the the url, let's write a function that will parse the url, extract the data and store it in a dataframe.\n",
    "\n",
    "In order to get a proper data set, we need to parse more than one page. So let's write the code to parse first 10 pages and store them in the same dataframe using <b>pd.concat</b> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse(url):\n",
    "    time.sleep(10)\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "    df = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\"])\n",
    "    for each in soup.find_all(class_= \"result\" ):\n",
    "        time.sleep(10)\n",
    "        try: \n",
    "            title = each.find(class_='jobtitle').text.replace('\\n', '')\n",
    "        except:\n",
    "            title = 'None'\n",
    "        try:\n",
    "            location = each.find('span', {'class':\"location\" }).text.replace('\\n', '')\n",
    "        except:\n",
    "            location = 'None'\n",
    "        try: \n",
    "            company = each.find(class_='company').text.replace('\\n', '')\n",
    "        except:\n",
    "            company = 'None'\n",
    "        try:\n",
    "            salary = each.find('span', {'class':'no-wrap'}).text.replace('\\n', '')\n",
    "        except:\n",
    "            salary = 'None'\n",
    "        #synopsis = each.find('span', {'class':'summary'}).text.replace('\\n', '')\n",
    "        df = df.append({'Title':title, 'Location':location, 'Company':company, 'Salary':salary}, ignore_index=True)\n",
    "    return df\n",
    "df1 = parse(url)\n",
    "url_1 = url + \"&start=20\"\n",
    "url_2 = url + \"&start=40\"\n",
    "url_3 = url + \"&start=60\"\n",
    "url_4 = url + \"&start=80\"\n",
    "url_5 = url + \"&start=100\"\n",
    "url_6 = url + \"&start=120\"\n",
    "url_7 = url + \"&start=140\"\n",
    "url_8 = url + \"&start=160\"\n",
    "url_9 = url + \"&start=180\"\n",
    "url_10 = url + \"&start=200\"\n",
    "\n",
    "df2 = parse(url_1)\n",
    "time.sleep(10)\n",
    "df3 = parse(url_2)\n",
    "time.sleep(10)\n",
    "df4 = parse(url_3)\n",
    "time.sleep(10)\n",
    "df5 = parse(url_4)\n",
    "time.sleep(10)\n",
    "df6 = parse(url_5)\n",
    "time.sleep(10)\n",
    "df7 = parse(url_6)\n",
    "time.sleep(10)\n",
    "df8 = parse(url_7)\n",
    "time.sleep(10)\n",
    "df9 = parse(url_8)\n",
    "time.sleep(10)\n",
    "df10 = parse(url_9)\n",
    "time.sleep(10)\n",
    "df11 = parse(url_10)\n",
    "\n",
    "data1 = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11], axis=0, ignore_index=True)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv('data/data-raw-'+job_title_us+'-'+str(d2)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the dataset, we can see that it has a lot of stuff that we don't need like \\n, $ etc. So let's clean up the data.\n",
    "\n",
    "When we check the data type of data1, we can see that <b>Salary</b> is stored as an object. In addition to that, Salary is defined as per year, per month, per week and per hour. In order for our analysis to work, we need to have salary displayed as per year and the column salary itself should be either <b>*float*</b> or <b>*int*</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.dtypes # here, salary is an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_year_temp = data1[data1['Salary'].str.contains(' a year')].reset_index(drop=True)\n",
    "data_sal_year_1a = data_sal_year_temp.replace(' a year', '', regex=True) # replace 'a year'\n",
    "data_sal_year_1 = data_sal_year_1a.replace('\\n', '', regex=True) # replace \\n\n",
    "data_sal_year_2 = data_sal_year_1.replace('[\\$,)]', '', regex=True) # replace $\n",
    "data_sal_year_3a = data_sal_year_2['Salary'].str.split('-', expand=True)\n",
    "data_sal_year_3a.iloc[:, :] = data_sal_year_3a.iloc[:, :].astype('float')\n",
    "data_sal_year_3b = data_sal_year_3a.sum(axis=1).astype('float')\n",
    "data_sal_year_3c = (data_sal_year_3b/2)\n",
    "data_sal_year_3 = data_sal_year_3c.astype('int')\n",
    "data_sal_year_5 = pd.concat([data_sal_year_2, data_sal_year_3], axis=1, sort=True)\n",
    "data_sal_year_6 = data_sal_year_5.drop(['Salary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment takes the salary per month and converts it to per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_month_temp = data1[data1['Salary'].str.contains(' a month')].reset_index(drop=True)\n",
    "data_sal_month_1a = data_sal_month_temp.replace(' a month', '', regex=True)\n",
    "data_sal_month_1 = data_sal_month_1a.replace('\\n', '', regex=True)\n",
    "data_sal_month_2 = data_sal_month_1.replace('[\\$,)]', '', regex=True)\n",
    "data_sal_month_3a = data_sal_month_2['Salary'].str.split('-', expand=True)\n",
    "data_sal_month_3a.iloc[:, :] = data_sal_month_3a.iloc[:, :].astype('float')\n",
    "data_sal_month_3b = data_sal_month_3a.sum(axis=1).astype('float')\n",
    "data_sal_month_3c = (data_sal_month_3b/2) * 12\n",
    "data_sal_month_3 = data_sal_month_3c.astype('int')\n",
    "data_sal_month_5 = pd.concat([data_sal_month_2, data_sal_month_3], axis=1, sort=True)\n",
    "data_sal_month_6 = data_sal_month_5.drop(['Salary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment takes the salary per week and converts it to per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_week_temp = data1[data1['Salary'].str.contains(' a week')].reset_index(drop=True)\n",
    "data_sal_week_1a = data_sal_week_temp.replace(' a week', '', regex=True)\n",
    "data_sal_week_1 = data_sal_week_1a.replace('\\n', '', regex=True)\n",
    "data_sal_week_2 = data_sal_week_1.replace('[\\$,)]', '', regex=True)\n",
    "data_sal_week_3a = data_sal_week_2['Salary'].str.split('-', expand=True)\n",
    "data_sal_week_3a.iloc[:, :] = data_sal_week_3a.iloc[:, :].astype('float')\n",
    "data_sal_week_3b = data_sal_week_3a.sum(axis=1).astype('float')\n",
    "data_sal_week_3c = (data_sal_week_3b/2) * 52\n",
    "data_sal_week_3 = data_sal_week_3c.astype('int')\n",
    "data_sal_week_5 = pd.concat([data_sal_week_2, data_sal_week_3], axis=1, sort=True)\n",
    "data_sal_week_6 = data_sal_week_5.drop(['Salary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment takes the salary per hour and converts it to per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_hour_temp = data1[data1['Salary'].str.contains(' an hour')].reset_index(drop=True)\n",
    "data_sal_hour_1a = data_sal_hour_temp.replace(' an hour', '', regex=True)\n",
    "data_sal_hour_1 = data_sal_hour_1a.replace('\\n', '', regex=True)\n",
    "data_sal_hour_2 = data_sal_hour_1.replace('[\\$,)]', '', regex=True)\n",
    "data_sal_hour_3a = data_sal_hour_2['Salary'].str.split('-', expand=True)\n",
    "data_sal_hour_3a.iloc[:, :] = data_sal_hour_3a.iloc[:, :].astype('float')\n",
    "data_sal_hour_3b = data_sal_hour_3a.sum(axis=1).astype('float')\n",
    "data_sal_hour_3c = (data_sal_hour_3b/2) * 40 * 52\n",
    "data_sal_hour_3 = data_sal_hour_3c.astype('int')\n",
    "data_sal_hour_5 = pd.concat([data_sal_hour_2, data_sal_hour_3], axis=1, sort=True)\n",
    "data_sal_hour_6 = data_sal_hour_5.drop(['Salary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the rows that don't have any salary mentioned and save them in <b>data_sal_none_temp</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_none_temp = data1[data1['Salary'].str.contains('None')].reset_index(drop=True)\n",
    "data_sal_none_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring together our newly formed dataset and save it as <b>data_sal_all</b> and rename the table header as <b>Title</b>, <b>Location</b>, <b>Company</b> and <b>Salary</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_all = pd.concat([data_sal_year_6, data_sal_month_6, data_sal_week_6, data_sal_hour_6], axis=0, sort=True)\n",
    "data_sal_all.columns = ['Title', 'Location', 'Company', 'Salary']\n",
    "data_sal_all.sort_values(by='Salary',ascending=True, inplace=True)\n",
    "data_sal_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's join the two data sets, <b>data_sal_all</b> and <b>data_sal_none_temp</b> and save them in <b>data_sal_all_1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_all_1 = pd.concat([data_sal_all, data_sal_none_temp], axis=0, sort=True)\n",
    "data_sal_all_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the shape of <b>data_sal_all_1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_all_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of <b>data_sal_all_1</b> is <b>(283, 5)</b> which is same as the shape of <b>data1</b>.\n",
    "This proves that we successfully managed to merge the 2 datasets without missing any row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replace <b>'None'</b> values with <b>np.nan</b> and change the data type of <b>Salary</b> to *float*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data5a = data_sal_all_1.replace('None',np.nan, regex=True)\n",
    "data5a['Salary'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the mean of the the cell Sa;ary using the .mean() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = data5a['Salary'].mean()\n",
    "mean_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now replace the np.nan cells with the mean salary obtained in the previous step.\n",
    "\n",
    "Let's also change the datatype to *int*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data5b = data5a['Salary'].replace(np.nan, mean_1)\n",
    "data5ba = data5b.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concact this salary to our previous dataframe <b>data_sal_all_1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5c = pd.concat([data_sal_all_1, data5ba], axis=1)\n",
    "data5c.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two Salary columns in our dataframe, so we rename the old one to Salary_1 and drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5c.columns = ['Company', 'Location', 'Salary_1', 'Title', 'Salary']\n",
    "data5d = data5c.drop(['Salary_1'], axis=1)\n",
    "data5d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5d.to_csv('data/data-'+job_title_us+'-'+str(d2)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's procede with the cleanup of our dataset. Let's drop the cells that don't have any location as it won't be useful to us in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6a = data5d.replace('None',np.nan, regex=True)\n",
    "data6 = data6a[pd.notnull(data5d[\"Location\"])]\n",
    "data6 = data6.reset_index(drop=True)\n",
    "data7 = data6.groupby('Location').count()\n",
    "sort_data = data7.sort_values('Title',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, <b>sort_data</b> has a location <b>Canada</b>. We need to drop that row. Since we made our code flexible by asking the user to input his job title, this field might not show up in other job titles. Hence we need to write the code that will check if this field is there, and if it, drop it, else procede without doing anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_data_canada_1 = sort_data.reset_index()\n",
    "sort_data_canada_2 = sort_data_canada_1.iloc[:, 0]\n",
    "sort_data_canada_3 = sort_data_canada_2.isin(['Canada'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sort_data_canada_3.any() == True:\n",
    "    sort_data_1 = sort_data.drop(['Canada'], axis=0)\n",
    "else:\n",
    "    sort_data_1 = sort_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_data_2 = sort_data_1.reset_index()\n",
    "sort_data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the location into <b>City</b> and <b>Provience</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = sort_data_2['Location'].str.split(',', expand=True)\n",
    "data1.columns = ['City', 'Provience']\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge this new data with the previous data and drop the column <b>Location</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.concat([data1, sort_data_2], axis=1, sort=False)\n",
    "data3 = data2.drop(['Location'], axis=1)\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which <b>Provience</b> has the maximum number of jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = data3.groupby('Provience')['Title'].apply(lambda x: ', '.join(x.astype(str))).reset_index()\n",
    "data5 = data4['Title'].str.split(',', expand=True)\n",
    "data5.iloc[:, :] = data5.iloc[:, :].astype(float)\n",
    "data5['Total'] = data5.sum(axis=1).astype(int)\n",
    "data8 = data5.loc[:, 'Total']\n",
    "data9 = data4.loc[:, 'Provience']\n",
    "data10 = pd.concat([data9, data8], axis=1, sort=True)\n",
    "data10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the dataframe by which <b>City</b> has the maximum number of jobs and arrange it in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11 = sort_data_1\n",
    "data11.sort_values(by='Title', ascending=True, inplace=True)\n",
    "data12 = data11.loc[:, 'Title']\n",
    "data13 = data12.tail(15)\n",
    "data13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure we don't have any duplicated in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data5d.drop_duplicates(keep=False,inplace=True)\n",
    "data5d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, many job titles have the same salary listed. For our last step, data visualization, we don’t need same salaries for different job titles, we just need one. So we group them by salary and then sort them in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5e = data5d.groupby('Salary')['Company'].apply(' '.join).reset_index()\n",
    "data5e.sort_values(by='Salary', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_all_1 = data5e.loc[:,'Salary']\n",
    "data_sal_all_2 = data_sal_all_1.tail(15)\n",
    "data_sal_all_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot which city has the maximum number of jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data13.plot(kind='barh', figsize=(12, 12), color='steelblue')\n",
    "plt.xlabel('Number of jobs')\n",
    "plt.title(job_title_1+' jobs in Canada '+str(d2))\n",
    "\n",
    "for index, value in enumerate(data13): \n",
    "    label = format(int(value), ',')\n",
    "    \n",
    "    plt.annotate(label, xy=(value - 2, index - 0.10), color='white')\n",
    "plt.savefig('images/'+job_title_us+'_jobs_in_Canada-'+str(d2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the salary range as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sal_all_2.plot(kind='barh', figsize=(15, 15), color='grey')\n",
    "plt.xlabel('Salary')\n",
    "plt.title(job_title_1+' salary in Canada '+str(d2))\n",
    "\n",
    "for index, value in enumerate(data_sal_all_2): \n",
    "    label = format(int(value), ',') # format int with commas\n",
    "    \n",
    "    plt.annotate(label, xy=(value - 12000, index - 0.10), color='white')\n",
    "plt.savefig('images/'+job_title_us+'_salary_in_Canada-'+str(d2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
